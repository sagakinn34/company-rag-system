#!/usr/bin/env python3
"""
ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from vector_db_processor import VectorDBProcessor
import json
import time
from datetime import datetime

def load_collected_data():
    """åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    try:
        # æœ€æ–°ã®çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        stats_files = [f for f in os.listdir('./data/results/') if f.startswith('collection_stats_')]
        if not stats_files:
            print("âŒ çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        latest_stats = sorted(stats_files)[-1]
        stats_path = f'./data/results/{latest_stats}'
        
        with open(stats_path, 'r', encoding='utf-8') as f:
            stats = json.load(f)
            
        # å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        timestamp = latest_stats.replace('collection_stats_', '').replace('.json', '')
        data_file = f'./data/results/all_documents_{timestamp}.json'
        
        if not os.path.exists(data_file):
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_file}")
            return None
            
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(data)}ä»¶")
        return data
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def process_in_batches(data, batch_size=20):
    """ãƒãƒƒãƒå‡¦ç†ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    if not data:
        print("âŒ å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return False
        
    try:
        processor = VectorDBProcessor()
        
        total_batches = (len(data) + batch_size - 1) // batch_size
        print(f"ğŸ“Š {len(data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’{batch_size}ä»¶ãšã¤{total_batches}ãƒãƒƒãƒã§å‡¦ç†ã—ã¾ã™")
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            print(f"\nğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†ä¸­... ({len(batch)}ä»¶)")
            
            # ãƒãƒƒãƒå‡¦ç†
            processor.add_documents(batch)
            
            print(f"âœ… ãƒãƒƒãƒ {batch_num} å®Œäº†")
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾ã®ãŸã‚ã®çŸ­ã„ä¼‘æ†©
            time.sleep(1)
            
            # é€²æ—ç¢ºèª
            current_count = processor.collection.count()
            print(f"ğŸ“Š ç¾åœ¨ã®DBæ–‡æ›¸æ•°: {current_count}ä»¶")
        
        final_count = processor.collection.count()
        print(f"\nğŸ‰ å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆå®Œäº†ï¼")
        print(f"ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ–‡æ›¸æ•°: {final_count}ä»¶")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    print("ğŸ”„ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = load_collected_data()
    if not data:
        return
    
    # ãƒãƒƒãƒå‡¦ç†ã§çµ±åˆ
    success = process_in_batches(data, batch_size=15)  # ã•ã‚‰ã«å°ã•ãªãƒãƒƒãƒ
    
    if success:
        print("\nâœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨æº–å‚™å®Œäº†")
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿çµ±åˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
